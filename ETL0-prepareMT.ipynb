{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76c381c-2210-409b-a21b-8e497013f184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import time\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331d36b8",
   "metadata": {},
   "source": [
    "### Cloud Environment set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ed36b4-bfde-4bda-b746-c7d4c108c6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name SageMaker-Climadata to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::669361146924:role/SageMaker-Climadata\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role() \n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9867417b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public ip: 18.184.155.204\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 18.184.155.204/32, TCP, from port: 3306, to port: 3306, ALLOW\" already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m security_group_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msg-00d2d7fadeb4493a7\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Add the public IP address to the security group\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mec2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthorize_security_group_ingress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mGroupId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecurity_group_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mIpProtocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtcp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFromPort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3306\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Change this to your database's port\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mToPort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3306\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Change this to your database's port\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCidrIp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpublic_ip_address\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    550\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1007\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 18.184.155.204/32, TCP, from port: 3306, to port: 3306, ALLOW\" already exists"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# Get the public IP address\n",
    "public_ip_address = requests.get('https://api.ipify.org').text\n",
    "\n",
    "print(\"public ip:\", public_ip_address)\n",
    "\n",
    "# Create a boto3 client\n",
    "ec2 = boto3.client('ec2')\n",
    "\n",
    "# The id of the security group to modify\n",
    "security_group_id = ''\n",
    "\n",
    "# Add the public IP address to the security group\n",
    "ec2.authorize_security_group_ingress(\n",
    "    GroupId=security_group_id,\n",
    "    IpProtocol='tcp',\n",
    "    FromPort=3306,  # Change this to your database's port\n",
    "    ToPort=3306,  # Change this to your database's port\n",
    "    CidrIp=f'{public_ip_address}/32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71d0104c-bf4d-4df8-8c17-5c3556920c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting metadata from s3\n",
    "import boto3\n",
    "\n",
    "# Set up AWS credentials and region\n",
    "aws_access_key_id = ''\n",
    "aws_secret_access_key = ''\n",
    "region_name = 'eu-central-1'\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "s3 = session.client('s3')\n",
    "s3.download_file('climabucket.test', 'Metadata-DWD.csv', 'Metadata-DWD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502d7a34-1834-4d67-a349-6355e581f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into a pandas DataFrame\n",
    "os.chdir('/home/ec2-user/SageMaker/')\n",
    "\n",
    "metadata = pd.read_csv('Metadata-DWD.csv', sep=',', header=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892fe657",
   "metadata": {},
   "source": [
    "### Access DWD data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1adc011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air_temperature/recent/', 'cloud_type/recent/', 'cloudiness/recent/', 'dew_point/recent/', 'extreme_wind/recent/', 'moisture/recent/', 'precipitation/recent/', 'pressure/recent/', 'soil_temperature/recent/', 'solar/', 'sun/recent/', 'visibility/recent/', 'weather_phenomena/recent/', 'wind/recent/', 'wind_synop/recent/']\n"
     ]
    }
   ],
   "source": [
    "#getting the names of all the subfolders\n",
    "\n",
    "base_url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/\"\n",
    "\n",
    "# Get the base page\n",
    "response = requests.get(base_url)\n",
    "directories = []\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        # Check if the link is a directory and not a parent directory link\n",
    "        if href.endswith('/') and not href.startswith('../'):\n",
    "            directories.append(href + \"recent/\")\n",
    "else:\n",
    "    print(f\"Failed to get page from {base_url}. Status code: {response.status_code}\")\n",
    "\n",
    "# replace solar/recent with solar\n",
    "directories[directories.index('solar/recent/')] = 'solar/'\n",
    "print(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26bbcb3a-03d4-48f9-80be-fe0d9c7d87de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get a list of desired files from a directory, metadata df is required\n",
    "# IMPORTANT: this relies on naming convention in DWD that id is 5 digits starting from 15th from\n",
    "def get_files(url, metadata):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        all_names = [link.get('href') for link in soup.find_all('a')]\n",
    "        # Get unique station ids as strings, padded with zeros\n",
    "        station_ids = [str(id).zfill(5) for id in metadata['Stations_id'].unique()]\n",
    "        # Output only those files that contain station id from metadata \n",
    "        desired_names = []\n",
    "\n",
    "        for name in all_names:\n",
    "            if '_akt' in name:\n",
    "                substring = name[name.index('_akt')-5:name.index('_akt')]\n",
    "                for id in station_ids:\n",
    "                    if substring == id:\n",
    "                        desired_names.append(name)    \n",
    "        return desired_names\n",
    "    else:\n",
    "        print(f\"Failed to get file list from {url}. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "   \n",
    "def download_and_extract_file(url, file):\n",
    "    file_url = url + file\n",
    "    response = requests.get(file_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "            zip_ref.extractall()\n",
    "\n",
    "\n",
    "def collapse_data_from_folder(folder_name):\n",
    "    dir_df = pd.DataFrame()\n",
    "   \n",
    "    for file in os.listdir():\n",
    "        if file.startswith('produkt') and file.endswith('txt'):\n",
    "            incriment = pd.read_csv(file, sep=';', encoding='ISO-8859-1')\n",
    "            dir_df = pd.concat([dir_df, incriment])\n",
    "        else: pass\n",
    "\n",
    "    return dir_df\n",
    "\n",
    "\n",
    "def proc_measurements(df):\n",
    "    #df['MESS_DATUM'] = pd.to_datetime(df['MESS_DATUM'], format='%Y%m%d%H')\n",
    "    #df.set_index('MESS_DATUM', inplace=True)\n",
    "    if 'eor' in df.columns:\n",
    "        df.drop(columns = ['eor'], inplace = True)\n",
    "    # drop values without quality flag\n",
    "    if 'QB' in df.columns:\n",
    "        df = df[df['QB'] != -999]\n",
    "        \n",
    "    # VERY IMPORTANT: SAMPLING OF WEATHER STATIONS OCCURS HERE \n",
    "    df = df[df['STATIONS_ID'].isin(metadata['Stations_id'])]\n",
    "    return df\n",
    "\n",
    "    \n",
    "#function tot turn a contents of whole folder into a dataframe\n",
    "#base_url is root if interesting web-data\n",
    "#base_homedir is the directory where you want to store all the sub-folders data\n",
    "#directory is the folder in dwd folder structure\n",
    "#metadata is a df with metadata\n",
    "def process_directory(base_url, base_homedir, directory, metadata):\n",
    "    # free directory adrees from /recent/ part\n",
    "    dir_i = directory.split('/')[0]\n",
    "    url = base_url + directory\n",
    "    files = get_files(url,metadata)\n",
    "    # creating a folder to store all the files from desired directory\n",
    "    os.chdir(base_homedir)\n",
    "    os.mkdir(dir_i)\n",
    "    os.chdir(dir_i)\n",
    "    for file in files:\n",
    "        download_and_extract_file(url, file)\n",
    "    # deleting all the metadaten. mind that they contain only the info on number of missing values \n",
    "# but you can calculate them yourself. -999 is a missing value\n",
    "    files = glob.glob('Metadaten*')\n",
    "    for file in files:\n",
    "        os.remove(file)\n",
    "    dir_df = collapse_data_from_folder(folder_name = dir_i)    \n",
    "    return dir_df #proc_measurements(dir_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9f41f7-f4c2-4509-b10a-ce2dcdd67a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8285448551177979\n",
      "2.163947343826294\n",
      "3.5218117237091064\n",
      "4.512378931045532\n",
      "5.5096471309661865\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/\"\n",
    "\n",
    "# Define the new directory to host all other folders\n",
    "path = \"/home/ec2-user/SageMaker/basis_23Feb\"\n",
    "\n",
    "# Create the directory\n",
    "#os.mkdir(path)\n",
    "project_wd = path\n",
    "\n",
    "my_timer = []\n",
    "my_timer.append(time.time())\n",
    "air_temp = process_directory(base_url, project_wd, directories[0], metadata)\n",
    "weather_phenomena = process_directory(base_url, project_wd, directories[12], metadata)\n",
    "my_timer.append(time.time())\n",
    "cloudiness = process_directory(base_url, project_wd, directories[2], metadata)\n",
    "cloud_type = process_directory(base_url, project_wd, directories[1], metadata)\n",
    "soil_temp = process_directory(base_url, project_wd, directories[8], metadata)\n",
    "my_timer.append(time.time())\n",
    "pressure = process_directory(base_url, project_wd, directories[7], metadata)\n",
    "precipitation = process_directory(base_url, project_wd, directories[6], metadata)\n",
    "my_timer.append(time.time())\n",
    "sun = process_directory(base_url, project_wd, directories[10], metadata)\n",
    "moisture = process_directory(base_url, project_wd, directories[5], metadata)\n",
    "my_timer.append(time.time())\n",
    "extreme_wind = process_directory(base_url, project_wd, directories[4], metadata)\n",
    "wind = process_directory(base_url, project_wd, directories[13], metadata)\n",
    "my_timer.append(time.time())\n",
    "\n",
    "# Assuming my_timer is your list\n",
    "differences = [t - my_timer[0] for t in my_timer]\n",
    "\n",
    "# Print the results\n",
    "for diff in differences:\n",
    "    print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94a71835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.738299608230591"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before opti\n",
    "'''\n",
    "0.0\n",
    "2.9098830223083496\n",
    "4.398086786270142\n",
    "9.135090827941895\n",
    "11.377177953720093\n",
    "13.093568325042725\n",
    "'''\n",
    "#after opti\n",
    "'''\n",
    "0.0\n",
    "0.8888800144195557\n",
    "2.3557307720184326\n",
    "3.2843453884124756\n",
    "4.091327667236328\n",
    "4.738299608230591\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eae5589-bda6-4733-aea3-1a1fddd56b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Stations_id', 'von_datum', 'bis_datum', 'Stationshoehe', 'geoBreite',\n",
      "       'geoLaenge', 'Stationsname', 'Bundeslandion_id', 'Unnamed: 8'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 399,  400,  403,  410,  420,  424,  427,  430,  433, 3987])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''\n",
    "0 - ['air_temperature/recent/', \n",
    "1 -  'cloud_type/recent/', \n",
    "2 -  'cloudiness/recent/', \n",
    "3 -  'dew_point/recent/', \n",
    "4 -  'extreme_wind/recent/', \n",
    "5 -  'moisture/recent/', \n",
    "6 -  'precipitation/recent/', \n",
    "7 - 'pressure/recent/', \n",
    "8 - 'soil_temperature/recent/', \n",
    "9 - 'solar/', \n",
    "10- 'sun/recent/', \n",
    "11 - 'visibility/recent/', \n",
    "12 - 'weather_phenomena/recent/', \n",
    "13 - 'wind/recent/', \n",
    "14 - 'wind_synop/recent/']\n",
    "'''\n",
    "print(metadata.columns)\n",
    "metadata['Stations_id'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc97e608-fd26-4b46-a15a-dd307db70931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Table': {'Arn': 'arn:aws:timestream:eu-central-1:669361146924:database/myclimab/table/new_table', 'TableName': 'new_table', 'DatabaseName': 'myclimab', 'TableStatus': 'ACTIVE', 'RetentionProperties': {'MemoryStoreRetentionPeriodInHours': 24, 'MagneticStoreRetentionPeriodInDays': 365}, 'CreationTime': datetime.datetime(2024, 1, 16, 21, 59, 56, 954000, tzinfo=tzlocal()), 'LastUpdatedTime': datetime.datetime(2024, 1, 16, 21, 59, 56, 954000, tzinfo=tzlocal()), 'MagneticStoreWriteProperties': {'EnableMagneticStoreWrites': False}, 'Schema': {'CompositePartitionKey': [{'Type': 'MEASURE'}]}}, 'ResponseMetadata': {'RequestId': '5XRHNMNVAJLDYJWKA3EDV5LFPQ', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '5XRHNMNVAJLDYJWKA3EDV5LFPQ', 'content-type': 'application/x-amz-json-1.0', 'content-length': '465', 'date': 'Tue, 16 Jan 2024 21:59:56 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a Timestream write client\n",
    "client = boto3.client('timestream-write')\n",
    "\n",
    "# Specify the database name and the new table name\n",
    "database_name = 'myclimab'\n",
    "new_table_name = 'new_table'  # replace with your new table name\n",
    "\n",
    "# Create the new table\n",
    "response = client.create_table(\n",
    "    DatabaseName=database_name,\n",
    "    TableName=new_table_name,\n",
    "    RetentionProperties={\n",
    "        'MemoryStoreRetentionPeriodInHours': 24,\n",
    "        'MagneticStoreRetentionPeriodInDays': 365\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df12b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
