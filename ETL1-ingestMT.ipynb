{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a0dff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "import zipfile\n",
    "import os\n",
    "import datetime\n",
    "import glob\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89b2982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is necessary to load metadata about stations to start working\n",
    "os.chdir('/home/ec2-user/SageMaker/')\n",
    "metadata = pd.read_csv('Metadata-DWD.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f7cf33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to access data from folder \n",
    "def collapse_data_from_folder():\n",
    "    dir_df = pd.DataFrame()\n",
    "   \n",
    "    for file in os.listdir():\n",
    "        if file.startswith('produkt') and file.endswith('txt'):\n",
    "            incriment = pd.read_csv(file, sep=';', encoding='ISO-8859-1')\n",
    "            dir_df = pd.concat([dir_df, incriment])\n",
    "        else: pass\n",
    "\n",
    "    return dir_df\n",
    "\n",
    "def proc_measurements(df):\n",
    "    df['MESS_DATUM'] = pd.to_datetime(df['MESS_DATUM'], format='%Y%m%d%H')\n",
    "    #df.set_index('MESS_DATUM', inplace=True)\n",
    "    if 'eor' in df.columns:\n",
    "        df.drop(columns = ['eor'], inplace = True)\n",
    "    return df\n",
    "#THIS function returns fully prepared dataframe by tag\n",
    "def prep_from_folder(tag):\n",
    "    os.chdir(f\"/home/ec2-user/SageMaker/basis/{tag}\")\n",
    "    df = collapse_data_from_folder()\n",
    "    df = proc_measurements(df)\n",
    "    \n",
    "    # VERY IMPORTANT: SAMPLING OF WEATHER STATIONS OCCURS HERE \n",
    "    df = df[df['STATIONS_ID'].isin(metadata['Stations_id'])]\n",
    "    # NAs are coded as -999 \n",
    "    df = df.replace('-999', np.nan).replace(-999, np.nan)\n",
    "    # filter out only measurements from 2024\n",
    "    df = df[df['MESS_DATUM'] > pd.to_datetime('2024010100', format='%Y%m%d%H')]    \n",
    "    return df\n",
    "\n",
    "#THIS function adapts to other folders like 16Feb\n",
    "def prep_from_anyfolder(folder, tag):\n",
    "    os.chdir(f\"/home/ec2-user/SageMaker/{folder}/{tag}\")\n",
    "    df = collapse_data_from_folder()\n",
    "    df = proc_measurements(df)\n",
    "    \n",
    "    # VERY IMPORTANT: SAMPLING OF WEATHER STATIONS OCCURS HERE \n",
    "    df = df[df['STATIONS_ID'].isin(metadata['Stations_id'])]\n",
    "    # NAs are coded as -999 \n",
    "    df = df.replace('-999', np.nan).replace(-999, np.nan)\n",
    "    # filter out only measurements from 2024\n",
    "    df = df[df['MESS_DATUM'] > pd.to_datetime('2024010100', format='%Y%m%d%H')]    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92540e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_temp = prep_from_anyfolder(\"basis_20Feb\",\"air_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dac174a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (8.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c16ca57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing mysql connector \n",
    "# this requires instaklling the package\n",
    "# !pip install mysql-connector-python\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "conn = mysql.connector.connect(\n",
    "    #database=\"climadb2\",\n",
    "    user=\"\",\n",
    "    password=\"\", \n",
    "    host=\".rds.amazonaws.com\",\n",
    "    port=\"3306\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8730edfb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('clima_integra',)\n",
      "('information_schema',)\n",
      "('mysql',)\n",
      "('performance_schema',)\n",
      "('sys',)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all the names of datbaseson the host\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SHOW DATABASES\")\n",
    "for db in cursor:\n",
    "    print(db)\n",
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "495c0f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: DWD, Length: 5995\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(\"USE clima_integra\")  \n",
    "cursor.execute(\"SHOW TABLES\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "for table in tables:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table[0]}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"Table: {table[0]}, Length: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e664f41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "02a05aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preparation \n",
    "# prepare the big pandas df to ingest the data into timestream\n",
    "# List of data types\n",
    "data_types = ['wind', 'sun', 'cloudiness', 'extreme_wind', 'air_temperature', 'moisture', 'precipitation', 'pressure', 'soil_temperature']\n",
    "# Create a dictionary that maps DWD Name to Measure name\n",
    "measure_name_dict = {\n",
    "    'TT_TU': 'air_temperature',\n",
    "    'RF_TU': 'air_temperature',\n",
    "    'R1': 'precipitation',\n",
    "    'WRTR': 'precipitation',\n",
    "    'V_N': 'cloudiness',\n",
    "    'SD_SO': 'sun',\n",
    "    'F': 'wind',\n",
    "    'D': 'wind',\n",
    "    'FX_911': 'extreme_wind',\n",
    "    'ABSF_STD': 'moisture',\n",
    "    'TF_STD': 'moisture',\n",
    "    'P': 'pressure',\n",
    "    'P0': 'pressure',\n",
    "    'V_TE002': 'soil_temperature',\n",
    "    'V_TE005': 'soil_temperature',\n",
    "    'V_TE010': 'soil_temperature',\n",
    "    'V_TE020': 'soil_temperature',\n",
    "    'V_TE050': 'soil_temperature',\n",
    "    'V_TE100': 'soil_temperature',\n",
    "}\n",
    "\n",
    "# Prepare dataframes and store in a dictionary\n",
    "dfs = {data_type: prep_from_anyfolder(\"basis_23Feb\",data_type) for data_type in data_types}\n",
    "\n",
    "# Concatenate all dataframes\n",
    "df = pd.concat(dfs.values(), axis=0)\n",
    "df.columns = df.columns.str.replace(\" \",\"\")\n",
    "\n",
    "columns_to_drop = ['V_N_I', 'VP_STD', 'P_STD', 'TT_STD', 'RF_STD', 'TD_STD', 'RS_IND']\n",
    "    \n",
    "    # Drop columns if they exist\n",
    "for column in columns_to_drop:\n",
    "    if column in df.columns:\n",
    "        df = df.drop(columns=[column])\n",
    " # Remove quality indicator\n",
    "df = df.loc[:, ~df.columns.str.startswith('QN_')]\n",
    "# the esssence of data integration: all measurements taken by the same station at the same time are considered synchronous \n",
    "df = df.groupby([\"MESS_DATUM\", 'STATIONS_ID']).first().reset_index()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d46ca85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min date: 2024-01-01 01:00:00\n",
      "Max date: 2024-02-21 23:00:00\n",
      "Max date to now : 4 days 14:54:54.607514\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "min_date = df['MESS_DATUM'].min()\n",
    "max_date = df['MESS_DATUM'].max()\n",
    "\n",
    "print(f\"Min date: {min_date}\")\n",
    "print(f\"Max date: {max_date}\")\n",
    "print(f\"Max date to now : {dt.now() - max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29dd653d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'MESS_DATUM' column to datetime\n",
    "df['MESS_DATUM'] = pd.to_datetime(df['MESS_DATUM'])\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df[df['MESS_DATUM'] > '2024-02-16']\n",
    "\n",
    "# Check the length\n",
    "length = len(filtered_df)\n",
    "\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96fc5f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715, 21)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5283082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get the current time\n",
    "now = datetime.now()\n",
    "\n",
    "# Calculate the time 27 hours ago\n",
    "time_51_hours_ago = now - timedelta(hours=51)\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_df = df[df['MESS_DATUM'] >= time_51_hours_ago]\n",
    "df = filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4dfbb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 6235\n",
      "After: 6235\n"
     ]
    }
   ],
   "source": [
    "# quality check if stations have been sampled\n",
    "print(\"Before:\", len(df))\n",
    "\n",
    "# Filter rows where 'STATIONS_ID' is in 'Stations_id' of metadata\n",
    "df = df[df['STATIONS_ID'].isin(metadata['Stations_id'])]\n",
    "\n",
    "print(\"After:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9377bb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 0.9990768432617188 bytes\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame\n",
    "memory_usage = df.memory_usage().sum()\n",
    "\n",
    "print(f\"Memory usage: {memory_usage/1024**2} megabytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9090e662",
   "metadata": {},
   "source": [
    "## Timestream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e6cca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for my big timestream ingestor. \n",
    "# i would need to authorize a boto3 session first, then metadata dataframe that i use to retrieve geo-location by station id\n",
    "\n",
    "os.chdir('/home/ec2-user/SageMaker')\n",
    "metadata = pd.read_csv('Metadata-DWD.csv', sep=',', header=0)\n",
    "metadata.index  = metadata['Stations_id']\n",
    "\n",
    "# Set up AWS credentials and region\n",
    "aws_access_key_id = ''\n",
    "aws_secret_access_key = ''\n",
    "region_name = 'eu-central-1'\n",
    "DATABASE_NAME = 'myclimab'\n",
    "TABLE_NAME = 'clima-integra'\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13d33cdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18f101a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESS_DATUM     datetime64[ns]\n",
      "STATIONS_ID             int64\n",
      "F                     float64\n",
      "D                     float64\n",
      "SD_SO                 float64\n",
      "V_N                   float64\n",
      "FX_911                float64\n",
      "TT_TU                 float64\n",
      "RF_TU                 float64\n",
      "ABSF_STD              float64\n",
      "TF_STD                float64\n",
      "R1                    float64\n",
      "WRTR                  float64\n",
      "P                     float64\n",
      "P0                    float64\n",
      "V_TE002               float64\n",
      "V_TE005               float64\n",
      "V_TE010               float64\n",
      "V_TE020               float64\n",
      "V_TE050               float64\n",
      "V_TE100               float64\n",
      "dtype: object\n",
      "(715, 21)\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66a2e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering data from 16th Feb\n",
    "# Convert your date string to a datetime object\n",
    "date_filter = pd.to_datetime('2024-02-16')\n",
    "\n",
    "# Filter the DataFrame\n",
    "df = df[df['MESS_DATUM'] > '2024-02-16']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e883ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715, 21)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c182381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to database myclimab table clima-integra\n",
      "processing 143 records from weather station  Berlin-Buch\n",
      "Processed 100 records. WriteRecords HTTPStatusCode: 200\n",
      "Processed 43 records. WriteRecords HTTPStatusCode: 200\n",
      "processing 143 records from weather station  Berlin-Dahlem-FU\n",
      "Processed 100 records. WriteRecords HTTPStatusCode: 200\n",
      "Processed 43 records. WriteRecords HTTPStatusCode: 200\n",
      "processing 143 records from weather station  Airport-Berlin-Brandenburg\n",
      "Processed 100 records. WriteRecords HTTPStatusCode: 200\n",
      "Processed 43 records. WriteRecords HTTPStatusCode: 200\n",
      "processing 143 records from weather station  Berlin-Tempelhof\n",
      "Processed 100 records. WriteRecords HTTPStatusCode: 200\n",
      "Processed 43 records. WriteRecords HTTPStatusCode: 200\n",
      "processing 143 records from weather station  Potsdam\n",
      "Processed 100 records. WriteRecords HTTPStatusCode: 200\n",
      "Processed 43 records. WriteRecords HTTPStatusCode: 200\n"
     ]
    }
   ],
   "source": [
    "# AND now the ingestion process begins\n",
    "def prepare_common_attributes_auto(name):\n",
    "    metastrip = metadata[metadata['Stationsname'] == name]\n",
    "    common_attributes = {\n",
    "        'Dimensions': [\n",
    "            {'Name': 'name', 'Value': metastrip['Stationsname'].item()},\n",
    "            {'Name': 'height', 'Value': str(metastrip['Stationshoehe'].item())},\n",
    "            {'Name': 'latt', 'Value': str(metastrip['geoBreite'].item())},\n",
    "            {'Name': 'long', 'Value': str(metastrip['geoLaenge'].item())}\n",
    "        ],    \n",
    "    }\n",
    "    return common_attributes\n",
    "# df goes into prepare records! \n",
    "# In the prepare_records function, assign the measure name to each record\n",
    "def prepare_records(i, df):\n",
    "    strip = df.iloc[i,:]\n",
    "    timestamp = str(int(strip['MESS_DATUM'].timestamp() * 1e9))  # Convert to nanoseconds\n",
    "\n",
    "    multi_val_list = []\n",
    "    measure_name = None\n",
    "    for dwd_name, measure in measure_name_dict.items():\n",
    "        if pd.notna(strip[dwd_name]):\n",
    "            record = {\n",
    "                'Name': dwd_name,\n",
    "                'Value': str(strip[dwd_name]),\n",
    "                'Type':  'DOUBLE'\n",
    "            }\n",
    "            multi_val_list.append(record)\n",
    "            measure_name = measure\n",
    "\n",
    "    record = {\n",
    "        'Time': timestamp,\n",
    "        'MeasureValues':multi_val_list,\n",
    "        'TimeUnit': 'NANOSECONDS',\n",
    "        'MeasureName': measure_name,\n",
    "        'MeasureValueType': 'MULTI'\n",
    "    }\n",
    "    return [record]\n",
    "\n",
    "def _print_rejected_records_exceptions(err):\n",
    "    print(\"RejectedRecords: \", err)\n",
    "    for rr in err.response[\"RejectedRecords\"]:\n",
    "        print(\"Rejected Index \" + str(rr[\"RecordIndex\"]) + \": \" + rr[\"Reason\"])\n",
    "        if \"ExistingVersion\" in rr:\n",
    "            print(\"Rejected record existing version: \", rr[\"ExistingVersion\"])\n",
    "\n",
    "def write_records(write_client, records, common_attributes):\n",
    "    try:\n",
    "        result = write_client.write_records(DatabaseName=DATABASE_NAME,\n",
    "                                            TableName=TABLE_NAME,\n",
    "                                            CommonAttributes=common_attributes,\n",
    "                                            Records=records)\n",
    "        status = result['ResponseMetadata']['HTTPStatusCode']\n",
    "        print(\"Processed %d records. WriteRecords HTTPStatusCode: %s\" %\n",
    "              (len(records), status))\n",
    "    except write_client.exceptions.RejectedRecordsException as err:\n",
    "        _print_rejected_records_exceptions(err)\n",
    "    except Exception as err:\n",
    "        print(\"Error:\", err)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"writing data to database {} table {}\".format(\n",
    "        DATABASE_NAME, TABLE_NAME))\n",
    "    session = boto3.Session()\n",
    "    write_client = session.client('timestream-write', config=Config(\n",
    "        read_timeout=20, max_pool_connections=5000, retries={'max_attempts': 10}))\n",
    "\n",
    "    log = {}\n",
    "    for _, row in metadata.iterrows():\n",
    "        station = row['Stationsname']\n",
    "        station_id = row['Stations_id']\n",
    "        common_attributes = prepare_common_attributes_auto(station)\n",
    "        # create slice dataframe from one station\n",
    "        station_data_frame = df[df['STATIONS_ID']==station_id]\n",
    "        records = []\n",
    "        n_rec = station_data_frame.shape[0]\n",
    "        log[station] = n_rec\n",
    "        print(\"processing\",n_rec,\"records from weather station \",station)\n",
    "        \n",
    "        # pass station df to create records \n",
    "        for i in range(n_rec) : \n",
    "            record = prepare_records(i, station_data_frame) # prepare from slice df\n",
    "            records.append(record[0])\n",
    "\n",
    "        # load data with batches to minimize costs \n",
    "        for i in range(0, len(records), 100):\n",
    "            batch = records[i:i+100]\n",
    "            try:\n",
    "                result = write_records(write_client, batch, common_attributes)\n",
    "            except ClientError as e:\n",
    "                log[f'Error_{station}_{i}'] = e \n",
    "                print('We have found an Error:', e)\n",
    "    return log \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mylog = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "985d8523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Berlin-Buch': 143,\n",
       " 'Berlin-Dahlem-FU': 143,\n",
       " 'Airport-Berlin-Brandenburg': 143,\n",
       " 'Berlin-Tempelhof': 143,\n",
       " 'Potsdam': 143}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9b5b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cursor.execute(\"DROP DATABASE clima_testerday\")\n",
    "cursor.execute(\"CREATE DATABASE clima_integra\")\n",
    "cursor.execute(\"USE clima_integra\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df01c1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index of the DataFrame\n",
    "metadata = metadata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6b59a",
   "metadata": {},
   "source": [
    "### preparation of  data for relational purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9215f680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MESS_DATUM', 'STATIONS_ID', 'F', 'D', 'SD_SO', 'V_N', 'FX_911',\n",
       "       'TT_TU', 'RF_TU', 'ABSF_STD', 'TF_STD', 'R1', 'WRTR', 'P', 'P0',\n",
       "       'V_TE002', 'V_TE005', 'V_TE010', 'V_TE020', 'V_TE050', 'V_TE100',\n",
       "       'Stationshoehe', 'geoBreite', 'geoLaenge', 'Stationsname'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_df = df.merge(metadata[['Stationshoehe','geoBreite', 'geoLaenge','Stationsname', 'Stations_id']], \n",
    "              how='left', \n",
    "              left_on='STATIONS_ID', \n",
    "              right_on='Stations_id')\n",
    "rel_df.drop('Stations_id', axis = 1, inplace = True)\n",
    "rel_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afaf1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_df = rel_df[['STATIONS_ID','MESS_DATUM','Stationsname','geoBreite', 'geoLaenge', 'Stationshoehe',   \n",
    "                 'F', 'D', 'TT_TU', 'RF_TU', 'R1','WRTR','SD_SO', \n",
    "                 'V_N', 'FX_911', 'ABSF_STD', 'TF_STD', 'P', 'P0',\n",
    "                'V_TE002', 'V_TE005', 'V_TE010', 'V_TE020', 'V_TE050', 'V_TE100']]\n",
    "rel_df = rel_df.replace({np.nan:None})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b6b2e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6235, 25)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf038892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 25)\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'MESS_DATUM' column to datetime\n",
    "rel_df['MESS_DATUM'] = pd.to_datetime(rel_df['MESS_DATUM'])\n",
    "\n",
    "# Create a time range for the entire day of '2024-02-16'\n",
    "start_time = pd.to_datetime('2024-02-16 00:00:00')\n",
    "end_time = pd.to_datetime('2024-02-16 01:00:00')\n",
    "\n",
    "# Filter the DataFrame\n",
    "rel_df = rel_df[(rel_df['MESS_DATUM'] >= start_time) & (rel_df['MESS_DATUM'] < end_time)]\n",
    "\n",
    "# Print the shape of the filtered DataFrame\n",
    "print(rel_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f72b1e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATIONS_ID</th>\n",
       "      <th>MESS_DATUM</th>\n",
       "      <th>Stationsname</th>\n",
       "      <th>geoBreite</th>\n",
       "      <th>geoLaenge</th>\n",
       "      <th>Stationshoehe</th>\n",
       "      <th>F</th>\n",
       "      <th>D</th>\n",
       "      <th>TT_TU</th>\n",
       "      <th>RF_TU</th>\n",
       "      <th>...</th>\n",
       "      <th>ABSF_STD</th>\n",
       "      <th>TF_STD</th>\n",
       "      <th>P</th>\n",
       "      <th>P0</th>\n",
       "      <th>V_TE002</th>\n",
       "      <th>V_TE005</th>\n",
       "      <th>V_TE010</th>\n",
       "      <th>V_TE020</th>\n",
       "      <th>V_TE050</th>\n",
       "      <th>V_TE100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5515</th>\n",
       "      <td>400</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>Berlin-Buch</td>\n",
       "      <td>52.6310</td>\n",
       "      <td>13.5021</td>\n",
       "      <td>60</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9.4</td>\n",
       "      <td>95.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5516</th>\n",
       "      <td>403</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>Berlin-Dahlem-FU</td>\n",
       "      <td>52.4537</td>\n",
       "      <td>13.3017</td>\n",
       "      <td>51</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>9.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.9</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1016.3</td>\n",
       "      <td>1006.6</td>\n",
       "      <td>None</td>\n",
       "      <td>7.2</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.1</td>\n",
       "      <td>6.8</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5517</th>\n",
       "      <td>427</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>Airport-Berlin-Brandenburg</td>\n",
       "      <td>52.3807</td>\n",
       "      <td>13.5306</td>\n",
       "      <td>46</td>\n",
       "      <td>4.1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>94.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1016.6</td>\n",
       "      <td>1010.5</td>\n",
       "      <td>None</td>\n",
       "      <td>7.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>433</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>Berlin-Tempelhof</td>\n",
       "      <td>52.4676</td>\n",
       "      <td>13.4020</td>\n",
       "      <td>48</td>\n",
       "      <td>2.3</td>\n",
       "      <td>160.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.6</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1016.5</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>None</td>\n",
       "      <td>8.2</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.5</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5519</th>\n",
       "      <td>3987</td>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>Potsdam</td>\n",
       "      <td>52.3812</td>\n",
       "      <td>13.0622</td>\n",
       "      <td>81</td>\n",
       "      <td>5.2</td>\n",
       "      <td>170.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>92.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1016.2</td>\n",
       "      <td>1004.5</td>\n",
       "      <td>None</td>\n",
       "      <td>7.4</td>\n",
       "      <td>8.1</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      STATIONS_ID MESS_DATUM                Stationsname  geoBreite  \\\n",
       "5515          400 2024-02-16                 Berlin-Buch    52.6310   \n",
       "5516          403 2024-02-16            Berlin-Dahlem-FU    52.4537   \n",
       "5517          427 2024-02-16  Airport-Berlin-Brandenburg    52.3807   \n",
       "5518          433 2024-02-16            Berlin-Tempelhof    52.4676   \n",
       "5519         3987 2024-02-16                     Potsdam    52.3812   \n",
       "\n",
       "      geoLaenge  Stationshoehe     F      D TT_TU  RF_TU  ... ABSF_STD TF_STD  \\\n",
       "5515    13.5021             60  None   None   9.4   95.0  ...      8.6    9.0   \n",
       "5516    13.3017             51  None   None   9.1  100.0  ...      8.9    9.1   \n",
       "5517    13.5306             46   4.1  160.0   9.3   94.0  ...      8.4    8.8   \n",
       "5518    13.4020             48   2.3  160.0  10.0   92.0  ...      8.6    9.3   \n",
       "5519    13.0622             81   5.2  170.0   9.6   92.0  ...      8.4    9.0   \n",
       "\n",
       "           P      P0 V_TE002 V_TE005 V_TE010 V_TE020 V_TE050 V_TE100  \n",
       "5515    None    None    None    None    None    None    None    None  \n",
       "5516  1016.3  1006.6    None     7.2     7.5     8.1     6.8     6.2  \n",
       "5517  1016.6  1010.5    None     7.5     8.0     8.1     6.7     6.1  \n",
       "5518  1016.5  1010.6    None     8.2     8.6     8.5     6.8     5.8  \n",
       "5519  1016.2  1004.5    None     7.4     8.1     8.2     6.5     5.6  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d32a97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of placeholders in SQL query: 25\n",
      "Number of elements in data_tuple: 25\n"
     ]
    }
   ],
   "source": [
    "# CREATE TABLE query\n",
    "table_query = \"\"\"\n",
    "    CREATE TABLE DWDtest1 (\n",
    "        id INT AUTO_INCREMENT,\n",
    "        station_id INT,\n",
    "        datetime DATETIME,\n",
    "        name VARCHAR(255),\n",
    "        lattitude FLOAT,\n",
    "        longitude FLOAT,\n",
    "        height FLOAT,\n",
    "        wind_speed FLOAT,\n",
    "        wind_direct FLOAT,\n",
    "        temp FLOAT,\n",
    "        humidity FLOAT,\n",
    "        prec_mm FLOAT,\n",
    "        prec_form INT,\n",
    "        sun_dur FLOAT,\n",
    "        cloudy TINYINT,\n",
    "        extreme_wind_speed INT,\n",
    "        humidity_abs FLOAT,\n",
    "        temp_wet_bulb FLOAT,\n",
    "        pres_sealvl FLOAT,\n",
    "        pres FLOAT,\n",
    "        soil_temp_2 FLOAT,\n",
    "        soil_temp_5 FLOAT,\n",
    "        soil_temp_10 FLOAT,\n",
    "        soil_temp_20 FLOAT,\n",
    "        soil_temp_50 FLOAT,\n",
    "        soil_temp_100 FLOAT,\n",
    "        PRIMARY KEY (id)\n",
    "    );\n",
    "\"\"\"\n",
    "\n",
    "# INSERT INTO query\n",
    "insert_query = '''\n",
    "    INSERT INTO DWDtest1 (\n",
    "        station_id, \n",
    "        datetime, \n",
    "        name, \n",
    "        lattitude, \n",
    "        longitude, \n",
    "        height, \n",
    "        wind_speed, \n",
    "        wind_direct, \n",
    "        temp, \n",
    "        humidity, \n",
    "        prec_mm, \n",
    "        prec_form, \n",
    "        sun_dur, \n",
    "        cloudy, \n",
    "        extreme_wind_speed, \n",
    "        humidity_abs, \n",
    "        temp_wet_bulb, \n",
    "        pres_sealvl, \n",
    "        pres, \n",
    "        soil_temp_2, \n",
    "        soil_temp_5, \n",
    "        soil_temp_10, \n",
    "        soil_temp_20, \n",
    "        soil_temp_50, \n",
    "        soil_temp_100\n",
    "    ) \n",
    "    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "'''\n",
    "\n",
    "\n",
    "#cursor.execute(table_query)\n",
    "# Insert data batch-wise\n",
    "batch_size = 1000\n",
    "batches = [rel_df[i:i + batch_size] for i in range(0, rel_df.shape[0], batch_size)]\n",
    "# Corrected data_tuples creation\n",
    "for batch in batches:\n",
    "    data_tuples = [tuple(row[col] for col in rel_df.columns) for index, row in batch.iterrows()]\n",
    "\n",
    "    # Debugging\n",
    "    print(f\"Number of placeholders in SQL query: {insert_query.count('%s')}\")\n",
    "    print(f\"Number of elements in data_tuple: {len(data_tuples[0])}\")\n",
    "    cursor.executemany(insert_query, data_tuples)\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daa29248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f2e88515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'your_table_name' and 'your_date_column' with your actual table name and date column name\n",
    "cursor.execute(\"DELETE FROM DWD WHERE datetime >= '2024-02-16'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08c682",
   "metadata": {},
   "source": [
    "Reporting tool to analyze the distribution among weather stations and dates range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2684b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_counts = {}\n",
    "\n",
    "for data_type, data_frame in dfs.items():\n",
    "    observation_counts[data_type] = data_frame.groupby('STATIONS_ID').size()\n",
    "\n",
    "# Print the results\n",
    "for data_type, counts in observation_counts.items():\n",
    "    print(f\"\\nMeasurement type: {data_type}\")\n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8de70fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame: wind\n",
      "Shape: (3597, 5)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n",
      "DataFrame: sun\n",
      "Shape: (3597, 4)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n",
      "DataFrame: cloudiness\n",
      "Shape: (3597, 5)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n",
      "DataFrame: extreme_wind\n",
      "Shape: (4796, 4)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n",
      "DataFrame: air_temperature\n",
      "Shape: (5995, 5)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n",
      "DataFrame: moisture\n",
      "Shape: (5990, 10)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n",
      "DataFrame: precipitation\n",
      "Shape: (5995, 6)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n",
      "DataFrame: pressure\n",
      "Shape: (4796, 5)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n",
      "DataFrame: soil_temperature\n",
      "Shape: (4796, 9)\n",
      "Earliest date: 2024-01-01 01:00:00\n",
      "Latest date: 2024-02-19 23:00:00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report shape and date range for each dataframe\n",
    "for name, dataframe in dfs.items():\n",
    "    print(f\"DataFrame: {name}\")\n",
    "    print(f\"Shape: {dataframe.shape}\")\n",
    "    print(f\"Earliest date: {dataframe['MESS_DATUM'].min()}\")\n",
    "    print(f\"Latest date: {dataframe['MESS_DATUM'].max()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdebcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
